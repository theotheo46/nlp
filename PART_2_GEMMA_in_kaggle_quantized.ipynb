{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5eQ5oBswDDU"
      },
      "source": [
        "# Урок 9. Часть 2\n",
        "\n",
        "1. Смотрим на работу **квантизованной** Gemma 2B.\n",
        "1. Решаем задачи с помощью **квантизованной** Gemma 2B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xO9j0DYwDDY"
      },
      "source": [
        "## Gemma 2B 4bit\n",
        "\n",
        "1. Ссылка на kaggle.com https://www.kaggle.com/models/google/gemma\n",
        "1. Ссылка на huggingface.co https://huggingface.co/google/gemma-1.1-2b-it\n",
        "1. Ссылка на блогпост https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "1. Ссылка на еще один блогпост https://kipp.ly/transformer-inference-arithmetic/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXA-kvEwwDDY"
      },
      "outputs": [],
      "source": [
        "! pip install accelerate\n",
        "! pip install -i https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "Tdh7lDxhwDDZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "\n",
        "# Загружаем модель\n",
        "model_path = \"/kaggle/input/gemma/transformers/1.1-2b-it/1/\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Добавляем конфигурацию квантизации (будем грузить в 4 битах)\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=quantization_config, revision=\"float16\", device_map=\"auto\").eval()\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj9ICevwwDDZ"
      },
      "outputs": [],
      "source": [
        "quantization_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPqA8ig9wDDa"
      },
      "source": [
        "Не все параметры модели загружаются в 4 битах, например, слои эмбеддингов и нормализации остаются в своих старых типах.\n",
        "\n",
        "Так как мы будем работать на гпу Nvidia Tesla P100, то скорость не вырастет - на этой ГПУ нет в железе встроенных оптимизаций для вычислений с низкой точностью, как например, на A100 или RTX4090."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxQgxittwDDa"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRmcJxmCwDDa"
      },
      "outputs": [],
      "source": [
        "model.lm_head.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVHQlg73wDDa"
      },
      "outputs": [],
      "source": [
        "# Токенизируем текст\n",
        "input_text = \"Write me a poem about Machine Learning.\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHep-KflwDDb"
      },
      "outputs": [],
      "source": [
        "# Первая генерация текста\n",
        "outputs = model.generate(**input_ids, max_new_tokens=20)\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFIX9fYzwDDb"
      },
      "outputs": [],
      "source": [
        "# Токенизируем как чат, модель училась общаться в формате диалога\n",
        "conversation = [{\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"}]\n",
        "input_ids = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "print(tokenizer.batch_decode(input_ids)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhs3cUn2wDDb"
      },
      "outputs": [],
      "source": [
        "# Вторая генерация текста\n",
        "generate_ids = model.generate(input_ids, max_new_tokens=20)\n",
        "\n",
        "print(tokenizer.batch_decode(generate_ids)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WplGbOeywDDb"
      },
      "outputs": [],
      "source": [
        "new_tokens = generate_ids[0, input_ids.shape[-1]:]\n",
        "\n",
        "print(tokenizer.decode(new_tokens, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA5ffL5PwDDc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "def measure_time(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()  # Record the start time\n",
        "        result = func(*args, **kwargs)  # Call the actual function\n",
        "        end_time = time.time()  # Record the end time\n",
        "        elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "        print(f\"Function '{func.__name__}' took {elapsed_time:.4f} seconds to complete.\\n\")\n",
        "        return result  # Return the result of the function\n",
        "    return wrapper\n",
        "\n",
        "# Напишем функцию для генерации текста - ответа на сообщение пользователя\n",
        "@measure_time\n",
        "@torch.inference_mode()\n",
        "def generate_text(prompt, **kwargs):\n",
        "    conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    input_ids = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "    generate_ids = model.generate(input_ids, **kwargs)\n",
        "    new_tokens = generate_ids[0, input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDVu-bAAwDDc"
      },
      "outputs": [],
      "source": [
        "print(generate_text(\"Hello!\", max_new_tokens=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiC4wEEWwDDc"
      },
      "source": [
        "## Температура\n",
        "\n",
        "Параметр температуры в LLM контролирует случайность предсказаний. Низкие значения температуры делают модель более детерминированной, в то время как высокие значения температуры делают ее более креативной и разнообразной. Вот общие рекомендации по настройке температуры:\n",
        "\n",
        "1. **Низкая температура (0.1 - 0.3)**:\n",
        "   - Результаты становятся более детерминированными и сфокусированными\n",
        "   - Хорошо подходит для задач, требующих точности, таких как ответы на фактические вопросы или генерация кода\n",
        "   - Меньше вероятность получения неожиданных или креативных ответов\n",
        "\n",
        "2. **Средняя температура (0.4 - 0.7)**:\n",
        "   - Балансирует между случайностью и детерминизмом\n",
        "   - Подходит для большинства задач общего назначения\n",
        "   - Обеспечивает связные, но несколько разнообразные результаты\n",
        "\n",
        "3. **Высокая температура (0.8 - 1.0)**:\n",
        "   - Увеличивает креативность и разнообразие в ответах\n",
        "   - Полезна для творческого письма, мозговых штурмов или создания поэзии\n",
        "   - Ответы могут быть менее предсказуемыми и более разнообразными\n",
        "\n",
        "4. **Очень высокая температура (выше 1.0)**:\n",
        "   - Может приводить к очень разнообразным и иногда бессмысленным результатам\n",
        "   - Обычно не рекомендуется для большинства задач, но можно экспериментировать для высоко креативных задач\n",
        "\n",
        "### Практические рекомендации:\n",
        "- **0.7**: Часто является хорошим выбором по умолчанию для сбалансированных результатов\n",
        "- **0.5**: Хорошо подходит для смешения креативности и связности\n",
        "- **0.2**: Идеально для задач, требующих высокой точности и последовательности\n",
        "\n",
        "Настройка температуры позволяет точно настроить поведение языковой модели для выполнения конкретных задач, поэтому эксперименты в этих диапазонах помогут достичь желаемого стиля вывода."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N769tKH0wDDd"
      },
      "outputs": [],
      "source": [
        "# Генерируем с разной температурой\n",
        "temperature_values = [0.1, 0.3, 0.5, 0.75, 1.0, 1.5]\n",
        "\n",
        "for temperature in temperature_values:\n",
        "    print(f\"{temperature=}\", end=\"\\n\\n\")\n",
        "    print(generate_text(\"Write me a haiku about deep learning\", max_new_tokens=100, temperature=temperature, do_sample=True))\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLGFWgivwDDd"
      },
      "source": [
        "## Саммаризация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw7cG28-wDDd"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "The temperature parameter in language models (LLMs) controls the randomness of the predictions. Lower temperatures make the model more deterministic, while higher temperatures make it more creative and diverse. Here are some general guidelines for setting the temperature:\n",
        "\n",
        "1. **Low Temperature (0.1 - 0.3)**:\n",
        "   - Results in more deterministic and focused outputs.\n",
        "   - Good for tasks requiring precision, such as factual answers or code generation.\n",
        "   - The model is less likely to produce unexpected or creative responses.\n",
        "\n",
        "2. **Medium Temperature (0.4 - 0.7)**:\n",
        "   - Balances between randomness and determinism.\n",
        "   - Suitable for most general-purpose tasks.\n",
        "   - Produces coherent yet somewhat varied outputs.\n",
        "\n",
        "3. **High Temperature (0.8 - 1.0)**:\n",
        "   - Increases creativity and diversity in responses.\n",
        "   - Useful for creative writing, brainstorming, or generating poetry.\n",
        "   - Outputs may be less predictable and more varied.\n",
        "\n",
        "4. **Very High Temperature (above 1.0)**:\n",
        "   - Can produce highly diverse and sometimes nonsensical outputs.\n",
        "   - Generally not recommended for most tasks but can be experimented with for highly creative tasks.\n",
        "\n",
        "### Practical Recommendations:\n",
        "- **0.7**: Often a good default choice for balanced outputs.\n",
        "- **0.5**: Good for a mix of creativity and coherence.\n",
        "- **0.2**: Ideal for tasks requiring high accuracy and consistency.\n",
        "\n",
        "Adjusting the temperature allows you to fine-tune the behavior of the language model to suit specific needs, so experimenting within these ranges can help you achieve the desired output style.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Summarize the following text in 2-3 sentences, capturing the main points and key details while maintaining coherence and accuracy. Ensure the summary is concise and informative.\n",
        "\n",
        "'''\n",
        "{text}\n",
        "'''\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-71JoYPwDDd"
      },
      "outputs": [],
      "source": [
        "print(generate_text(prompt, temperature=0.2, do_sample=True, max_new_tokens=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4xKwtbfwDDd"
      },
      "source": [
        "## Определение тональности текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLmuLenBwDDd"
      },
      "outputs": [],
      "source": [
        "text = \"This new GPT4o is a complete disaster. It's slow, inaccurate, and difficult to use. I hate it very much, the Google's Gemma is sooo better.\"\n",
        "\n",
        "prompt = f\"\"\"Determine the sentiment of this text. Return only sentiment.\n",
        "\n",
        "'''\n",
        "{text}\n",
        "'''\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGyld2bXwDDe"
      },
      "outputs": [],
      "source": [
        "print(generate_text(prompt, temperature=0.2, do_sample=True, max_new_tokens=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd9IkEvvwDDe"
      },
      "source": [
        "## Классификация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w26PaUVBwDDe"
      },
      "outputs": [],
      "source": [
        "text = \"The latest smartphone from Apple has received super positive reviews for its sleek design and powerful performance. But it is very expensive, so think for yourself!\"\n",
        "prompt = f\"\"\"Classify the following text into one of the categories: technology, sports, politics. Return only a category.\n",
        "\n",
        "'''\n",
        "{text}\n",
        "'''\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqAp4nCUwDDe"
      },
      "outputs": [],
      "source": [
        "print(generate_text(prompt, temperature=0.2, do_sample=True, max_new_tokens=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl5OZM_GwDDe"
      },
      "source": [
        "## Перевод"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxIwEZHiwDDe"
      },
      "outputs": [],
      "source": [
        "text = \"Биршерт Алексей Дмитриевич записывает лекцию и семинар для 9 занятия по курсу Глубинное обучение.\"\n",
        "source_language = \"russian\"\n",
        "target_language = \"spanish\"\n",
        "\n",
        "prompt = f\"\"\"Translate this text from {source_language} to {target_language}. Return only translation.\n",
        "\n",
        "'''\n",
        "{text}\n",
        "'''\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1ZX_Lt_wDDe"
      },
      "outputs": [],
      "source": [
        "translation = generate_text(prompt, temperature=0.7, do_sample=True, max_new_tokens=100)\n",
        "\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmIvaE_bwDDf"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"Translate this text from {target_language} to {source_language}. Return only translation.\n",
        "\n",
        "'''\n",
        "{translation}\n",
        "'''\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsYDfIuKwDDf"
      },
      "outputs": [],
      "source": [
        "back_translation = generate_text(prompt, temperature=0.7, do_sample=True, max_new_tokens=100)\n",
        "\n",
        "print(back_translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRoc3k2nwDDf"
      },
      "source": [
        "## Ответы на вопросы по тексту\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG4HMO34wDDf"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Jason Statham is an English actor and former competitive diver, best known for his roles in action-thriller films. Born on July 26, 1967, in Shirebrook, Derbyshire, England, Statham's journey to stardom is as remarkable as the characters he portrays. Before entering the film industry, he was a member of Britain's National Diving Squad for over a decade, competing at world championships and the Commonwealth Games. His rugged good looks and athletic build, combined with his martial arts skills, made him a natural fit for the action genre.\n",
        "\n",
        "Statham's film career began in 1998 when he was cast in Guy Ritchie's crime comedy \"Lock, Stock and Two Smoking Barrels.\" His performance caught the attention of audiences and critics alike, leading to a follow-up role in Ritchie's \"Snatch\" (2000), where he starred alongside Brad Pitt and Benicio del Toro. These early roles established him as a reliable actor capable of delivering tough, street-smart characters with a touch of humor.\n",
        "\n",
        "He gained international fame with his role as Frank Martin in \"The Transporter\" series (2002-2008), where he performed many of his own stunts, showcasing his skills in martial arts, driving, and combat. This franchise solidified his reputation as a top-tier action star. Statham continued to build on this success with roles in high-profile action films such as \"Crank\" (2006), \"War\" (2007), and \"Death Race\" (2008).\n",
        "\n",
        "In 2010, Statham joined the ensemble cast of \"The Expendables,\" alongside other action legends like Sylvester Stallone and Arnold Schwarzenegger. The film's success led to two sequels, further cementing his status in Hollywood. He also became part of the \"Fast & Furious\" franchise, debuting as the villain Deckard Shaw in \"Fast & Furious 6\" (2013) and reprising the role in subsequent films, including \"Furious 7\" (2015), \"The Fate of the Furious\" (2017), and the spin-off \"Hobbs & Shaw\" (2019).\n",
        "\n",
        "Statham's appeal lies in his ability to bring authenticity to his roles, performing stunts and fight scenes with a level of realism that resonates with audiences. Off-screen, he is known for his private and low-key lifestyle, a stark contrast to the high-octane characters he portrays. He has been in a long-term relationship with model Rosie Huntington-Whiteley, with whom he shares a son.\n",
        "\n",
        "Jason Statham's career is a testament to his versatility and dedication to his craft. From his beginnings as a competitive diver to becoming one of Hollywood's most bankable action stars, he has consistently delivered performances that are both compelling and entertaining. His contributions to the action genre have earned him a loyal fan base and a lasting legacy in the film industry.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s3Rth6HwDDg"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What were Jason Statham's professions before he became an actor?\",\n",
        "    \"Which film marked the beginning of Jason Statham's film career in 1998?\",\n",
        "    \"How did Jason Statham's role in 'The Transporter' series impact his career?\",\n",
        "    \"In which film franchise did Jason Statham play the character Deckard Shaw?\",\n",
        "    \"Who is Jason Statham's long-term partner, and do they have any children?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brvcJ8VMwDDg"
      },
      "outputs": [],
      "source": [
        "for question in questions:\n",
        "    print(question, end=\"\\n\\n\")\n",
        "\n",
        "    prompt = f\"\"\"Answer the question based on the context provided:\n",
        "\n",
        "QUESTION\n",
        "{question}\n",
        "\n",
        "'''CONTEXT\n",
        "{text}\n",
        "'''\n",
        "    \"\"\"\n",
        "    print(generate_text(prompt, temperature=0.7, do_sample=True, max_new_tokens=100))\n",
        "\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap-sOlZQwDDg"
      },
      "source": [
        "## Резюме\n",
        "\n",
        "1. Загрузили **квантизованную** модель Gemma с помощью библиотек `Transformers` и `bitsandbytes`\n",
        "2. Порешали все те же задачи с помощью **квантизованной** Gemma и сравнили с обычной"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "modelInstanceId": 22003,
          "sourceId": 26140,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}