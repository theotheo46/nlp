{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация текста с помощью RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поставим задачу: необходимо сгенерировать научный текст.\n",
    "\n",
    "В качестве датасета возьмем [выгрузку статей из Arxiv](https://www.kaggle.com/datasets/Cornell-University/arxiv). Используем версию 175 (4.13 GB).\n",
    "\n",
    "Метрику качества выберем \"на глаз\".\n",
    "\n",
    "Модель возьмем RNN, функция потерь будет кросс-энтропия, оптимизатор Adam.\n",
    "\n",
    "Модель будет предсказывать следующий токен. Кросс-энтропию будем считать над вероятностью реального токена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import wandb\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация\n",
    "\n",
    "В качестве токена в этой задаче будем брать **один символ**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2459562/2459562 [00:00<00:00, 5483201.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# BOS — символ начала текста, EOS — символ конца текста\n",
    "BOS, EOS = \" \", \"\\n\"\n",
    "\n",
    "lines = []\n",
    "# возьмем только каждую 10-ую строку в финальный датасет\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\", \"r\") as f:\n",
    "    for i, one_line in enumerate(tqdm.tqdm(f.readlines())):\n",
    "        if i % 10 == 0:\n",
    "            lines.append(one_line)\n",
    "with open(\"small-data.json\", \"w\") as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "data = pd.read_json(\"small-data.json\", lines=True)\n",
    "lines = (\n",
    "    data.apply(lambda row: (row[\"title\"] + \" ; \" + row[\"abstract\"])[:512], axis=1)\n",
    "    .apply(lambda line: BOS + line.replace(EOS, \" \") + EOS)\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Calculation of prompt diphoton production cross sections at Tevatron and   LHC energies ;   A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region o\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  97\n"
     ]
    }
   ],
   "source": [
    "tokens = {one_char for one_line in lines for one_char in one_line}\n",
    "\n",
    "tokens = sorted(tokens)\n",
    "n_tokens = len(tokens)\n",
    "print(\"n_tokens = \", n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {x: i for i, x in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Паддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 66, 67, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 66, 67, 66, 68, 66, 67, 66,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 66, 67, 68, 18, 19, 20, 21, 22, 23, 24, 25, 26, 17,  0]])\n"
     ]
    }
   ],
   "source": [
    "def to_tensor(\n",
    "    lines: list[str],\n",
    "    max_len: int | None = None,\n",
    "    pad: str = token_to_id[EOS],\n",
    "    dtype=torch.int64,\n",
    "):\n",
    "    max_len = max_len or max(map(len, lines))\n",
    "    lines_ix = torch.full([len(lines), max_len], pad, dtype=dtype)\n",
    "    for i in range(len(lines)):\n",
    "        line_ix = [token_to_id[x] for x in lines[i][:max_len]]\n",
    "        lines_ix[i, : len(line_ix)] = torch.tensor(line_ix)\n",
    "    return lines_ix\n",
    "\n",
    "\n",
    "print(to_tensor([\" abc\\n\", \" abacaba\\n\", \" abc1234567890\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False]])\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "def compute_mask(input_ix, eos_ix=token_to_id[EOS]):\n",
    "    return F.pad(\n",
    "        torch.cumsum(input_ix == eos_ix, dim=-1)[..., :-1] < 1,\n",
    "        pad=(1, 0, 0, 0),\n",
    "        value=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(compute_mask(to_tensor([\" hello there\\n\"], max_len=15)))\n",
    "print(compute_mask(to_tensor([\" hello there\\n\"], max_len=18)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "ref_rnn = nn.RNN(10, 32)\n",
    "print(ref_rnn.weight_hh_l0.shape)\n",
    "print(ref_rnn.weight_ih_l0.shape)\n",
    "print(ref_rnn.bias_hh_l0.shape)\n",
    "print(ref_rnn.bias_ih_l0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 12, 32]), torch.Size([1, 5, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyRnnLayer(nn.Module):\n",
    "    # tanh(W_x * x + W_h * h + b_x + b_h) = tanh(W_x * x + b_x     +     W_h * h + b_h)\n",
    "    def __init__(self, in_dim: int, hid_size: int):\n",
    "        super().__init__()\n",
    "        self.hid_size = hid_size\n",
    "        self.input_linear_layer = nn.Linear(\n",
    "            in_features=in_dim, out_features=hid_size, bias=True\n",
    "        )\n",
    "        self.hidden_state_linear_layer = nn.Linear(\n",
    "            in_features=hid_size, out_features=hid_size, bias=True\n",
    "        )\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: torch.Tensor | None = None):\n",
    "        assert x.ndim == 3  # (bs, n_tokens, token_dim)\n",
    "        if state is None:\n",
    "            state = torch.zeros((x.shape[0], self.hid_size), device=x.device)\n",
    "        states = [state]\n",
    "        for i in range(x.shape[1]):\n",
    "            states.append(\n",
    "                self.activation(\n",
    "                    self.input_linear_layer(x[:, i, :])\n",
    "                    + self.hidden_state_linear_layer(states[-1])\n",
    "                )\n",
    "            )\n",
    "        return torch.stack(states[1:]).permute((1, 0, 2)), states[-1][None, ...]\n",
    "\n",
    "\n",
    "my_rnn = MyRnnLayer(10, 32)\n",
    "test_input = torch.randn((5, 12, 10))\n",
    "test_output = my_rnn(test_input)\n",
    "test_output[0].shape, test_output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.testing import assert_close\n",
    "\n",
    "# Протестируем: наш слой должен давать те же результаты, что слой в pytorch\n",
    "ref_rnn = nn.RNN(10, 32, batch_first=True)\n",
    "with torch.no_grad():\n",
    "    my_rnn.hidden_state_linear_layer.weight.copy_(ref_rnn.weight_hh_l0.clone())\n",
    "    my_rnn.input_linear_layer.weight.copy_(ref_rnn.weight_ih_l0.clone())\n",
    "    my_rnn.hidden_state_linear_layer.bias.copy_(ref_rnn.bias_hh_l0.clone())\n",
    "    my_rnn.input_linear_layer.bias.copy_(ref_rnn.bias_ih_l0.clone())\n",
    "    for i in (0, 1):\n",
    "        actual = my_rnn(test_input)[i]\n",
    "        expected = ref_rnn(test_input)[i]\n",
    "        assert_close(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 токена\n",
    "# [0, 1, 2]\n",
    "# размерность эмбеддинга 2\n",
    "# 3x2 — матрица эмбеддингов\n",
    "\n",
    "[\n",
    "    [a, b],\n",
    "    [c, d],\n",
    "    [e, f],\n",
    "]\n",
    "\n",
    "# embbeding[0] --> [a, b]\n",
    "# embbeding[2] --> [e, f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 97])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_tokens: int = n_tokens, emb_size: int = 16, hid_size: int = 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=n_tokens, embedding_dim=emb_size)\n",
    "        self.rnn = MyRnnLayer(emb_size, hid_size)\n",
    "        # То же самое, что:\n",
    "        # self.rnn = nn.RNN(emb_size, hid_size, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=hid_size, out_features=n_tokens)\n",
    "\n",
    "    def forward(self, input_ix):\n",
    "        # input_ix -> (bs, n_tokens) = (1, 4)\n",
    "        rv = self.emb(input_ix)  # (1, 4, 16)\n",
    "        rv = self.rnn(rv)[0]  # (1, 4, 256)\n",
    "        rv = self.linear(rv)  # (1, 4, 256) @ (256, 97) = (1, 4, 97)\n",
    "        return rv\n",
    "\n",
    "\n",
    "model = RNNLanguageModel()\n",
    "model(to_tensor([\"in this article we\"])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь и генерация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция потерь\n",
    "Модель выдает вероятности каждого токена из словаря для каждого слова.\n",
    "Будем учить модель предсказывать следующий токен при условии всех предыдущих.\n",
    "\n",
    "![pic](./pic.jpg)\n",
    "\n",
    "Формула функции потерь:\n",
    "$$\n",
    "L = - \\cfrac{1}{N} \\sum_{i=1}^N \\ln p(x_t^{(i)} | x_{t-1}^{(i)}, \\dots, x_1^{(i)})\n",
    "$$\n",
    "где $N$ — размер батча.\n",
    "\n",
    "#### Генерация\n",
    "Поскольку модель выдает вектор вероятностей каждого токена в предложении, то для генерации одного текста поступим так:\n",
    "1. Берем вектор `(batch_size = 1, n_words, emb_dim)` из выхода модели, берем последнюю координату из `n_words`.\n",
    "2. Из полученного вектора `(1, emb_dim)` вероятностей решаем, как получить следующий токен.\n",
    "\n",
    "Существует два варианта того, как можно получить следующий токен:\n",
    "1. Отобрать тот, у кого самая большая вероятность — это **жадный выбор** (greedy sampling). Просто берем `argmax`.\n",
    "2. Взять случайный с учетом вероятностей.\n",
    "Модель выдает логиты — к ним применим softmax и получим вероятности.\n",
    "Пример сэмплирования: в векторе `[0.1, 0.3, 0.6]` первая координата будет выбрана с вероятностью 10%, вторая — с 30%, третья — с 60%.\n",
    "Это называется **случайное сэмплирование с учетом вероятностей**.\n",
    "3. Взять идею из п.2, но перед сэмплированием перевзвесить все логиты:\n",
    "$$\n",
    "p(l_i) = \\cfrac{\\exp(l_i / \\tau)}{\\sum_{i=1}^D \\exp(l_i / \\tau)}\n",
    "$$\n",
    "где $D$ — мощность словаря, число уникальных токенов, $l_i$ — логит для $i$-го токена, $\\tau$ — некоторый параметр, называемый **температурой**.\n",
    "\n",
    "Это называется **сэмплирование с температурой**.\n",
    "Варьируя температуру, можно либо перейти в жадный выбор, либо в равновероятный выбор токенов.\n",
    "\n",
    "Мы будем использовать п.1 и п.3 для сэмплирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model: nn.Module, input_ix: torch.Tensor, device: str = \"cpu\"):\n",
    "    input_ix = torch.as_tensor(input_ix, dtype=torch.int64)\n",
    "    input_ix = input_ix.to(device)\n",
    "\n",
    "    # (bs, sentence_length)\n",
    "    logits = model(input_ix[:, :-1])  # (bs, sentence_length, 97)\n",
    "    reference_answers = input_ix[:, 1:]  # (bs, sentence_length) [32, 13, 28, 1]\n",
    "    rv = torch.softmax(logits, 2)    # (bs, sentence_length, 97) --> (bs, sentence_length)\n",
    "    # (bs, sentence_length, 97)\n",
    "    # (bs, [0], [32])\n",
    "    # (bs, [1], [13])\n",
    "    # (bs, [2], [28])\n",
    "    # (bs, [3], [1])\n",
    "    # [:, :, 0]  # (a, b, c) -> (a, b)\n",
    "    rv = torch.gather(rv, 2, reference_answers[:, :, None]).squeeze(2)  # (bs, sentence_length)\n",
    "    rv = torch.log(rv)\n",
    "    # Потери считаем только для реальных токенов, паддинги не включаем.\n",
    "    # Для этого умножим на маску — она равна 1 для реальных токенов и 0 для паддингов.\n",
    "    rv = rv * compute_mask(input_ix)[:, 1:]\n",
    "    return -torch.sum(rv) / input_ix.shape[0]\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model: nn.Module,\n",
    "    prefix: str = BOS,\n",
    "    temperature: float = 1.0,\n",
    "    max_len: int = 100,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            probs = (\n",
    "                torch.softmax(model(to_tensor([prefix]).to(device))[0, -1], dim=-1)\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            if temperature == 0:\n",
    "                next_token = tokens[np.argmax(probs)]\n",
    "            else:\n",
    "                probs = np.array([p ** (1.0 / temperature) for p in probs])\n",
    "                probs /= sum(probs)\n",
    "                next_token = np.random.choice(tokens, p=probs)\n",
    "\n",
    "            prefix += next_token\n",
    "            if next_token == EOS or len(prefix) > max_len:\n",
    "                break\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение и результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malekseik1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aleksei/git/start-dl/start_dl/lesson_7/wandb/run-20240418_110759-6hzp5kgu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alekseik1/start-dl--lesson-7/runs/6hzp5kgu' target=\"_blank\">northern-flower-28</a></strong> to <a href='https://wandb.ai/alekseik1/start-dl--lesson-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alekseik1/start-dl--lesson-7' target=\"_blank\">https://wandb.ai/alekseik1/start-dl--lesson-7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alekseik1/start-dl--lesson-7/runs/6hzp5kgu' target=\"_blank\">https://wandb.ai/alekseik1/start-dl--lesson-7/runs/6hzp5kgu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksei/.cache/pypoetry/virtualenvs/start-dl-fEQaQ9Q8-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 3000/3000 [07:55<00:00,  6.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_loop(model: nn.Module, train_lines: list[str], device: str = \"cpu\"):\n",
    "    run = wandb.init(project=\"start-dl--lesson-7\")\n",
    "    clip_norm = 1e5\n",
    "    batch_size = 64\n",
    "    opt = Adam(model.parameters())\n",
    "    train_history = []\n",
    "    model.to(device)\n",
    "    text_table = wandb.Table(columns=[\"iteration\", \"text\"])\n",
    "    for i in tqdm.trange(len(train_history), 3000):\n",
    "        batch = to_tensor(sample(train_lines, batch_size)).to(device)\n",
    "        loss_i = compute_loss(model, batch, device=device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss_i.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        opt.step()\n",
    "\n",
    "        train_history.append((i, float(loss_i)))\n",
    "        wandb.log({\"loss\": loss_i.detach().cpu().item()})\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            for _ in range(3):\n",
    "                example = generate(model, temperature=0.5, device=device)\n",
    "                text_table.add_data(i, example)\n",
    "    run.log({\"train-samples\": text_table})\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_loop(model, lines, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      " On the controlled recently, an explained by the construction of the spin complex ;   The computations are been scale from the same of stable structure of $\\mathbb{Q}$ of structure of the magnetic fiel\n",
      "###\n",
      " Results of the singlet and analysis of the   interactions by a simulations of a function of a crystalling a first between the recent all several original models with a second problem of the discrete t\n",
      "###\n",
      " Structure with the properties of the research in the controlled to the dependent of the properties of the effective excitations of the magnetory (observing the contains the problem is a multiple and d\n",
      "###\n",
      " The condition of a structures of the superconductivity is investigated by the calculations of this studied the High measurements ;   We study the constraints in the sample of the statistics of the top\n",
      "###\n",
      " A Sells between the singular core interaction for a consistent and superconducting models are proposed in the complexity of the mass model to prove dependent structure of the Higgs between the converg\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print('###')\n",
    "    print(generate(model, temperature=0.5, max_len=200, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось вполне правдоподобно.\n",
    "\n",
    "В качестве упражнения попробуйте подвигать температуру:\n",
    "1. Слишком большая температура сделает токены более равновероятными. Из-за этого слова начнут превращаться в кашу.\n",
    "2. Слишком маленькая температура будет делать обратное — заострять пики распределения. Из-за этого сэмплирование превратится в жадное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Резюме\n",
    "\n",
    "1. Узнали, как решать задачу генерации текста с помощью RNN в PyTorch.\n",
    "2. Посмотрели, как формализовать задачу предсказания токена.\n",
    "3. Познакомились со стратегиями генерации токенов:\n",
    "    - жадное сэмплирование;\n",
    "    - случайное сэмплирование с температурой."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start-dl-fEQaQ9Q8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
