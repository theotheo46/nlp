{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d537e169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m160.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Installing collected packages: huggingface-hub, tokenizers\n",
      "Successfully installed huggingface-hub-0.30.2 tokenizers-0.21.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8422c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece, Unigram\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e433c5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>Если бесконечное количество российских футболистов запустить на бесконечное количество футбольных полей и дать им бесконечное количество времени, то один из них когда-нибудь забьёт гол.\n",
      "\n",
      "<|startoftext|>На чемпионат мира по футболу от России нужно Юлию Самойлову отправлять, хоть какая-то надежда на победу будет.\n",
      "\n",
      "<|startoftext|>В целях профилактики от всего весной следует есть много чеснока. От женщин, кстати, тоже помогает.\n",
      "\n",
      "<|startoftext|>На моих глазах как-то две девушки затаскивали кавказца в машину. Они худенькие, а он здоровый такой, никак не хотел в машину лезть. Они попросили у меня помощи, сказали, что собаку надо в ветклинику отвезти.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! tail -n 8 data/anek.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4723b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anek.txt', 'r') as f:\n",
    "    aneki = f.read().strip().replace('<|startoftext|>', '').split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55b40c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Только заметил, что слово \"п@рно\" набирается самими центральными клавишами. Как все продумано, блин!',\n",
       " 'Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!',\n",
       " '- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...',\n",
       " '- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От собственного храпа по крайней мере еще ни разу не просыпался.- Ну, так у жены спроси.- А жена и подавно не знает. У нее странная привычка после замужества возникла: как спать ложится - беруши вставляет.',\n",
       " 'Поссорилась с мужем. Пока он спал, я мысленно развелась с ним, поделила имущество, переехала, поняла, что жить без него не могу, дала последний шанс, вернулась. В итоге, ложусь спать уже счастливой женщиной.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aneki[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414dc14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число уникальных слов: 159497\n"
     ]
    }
   ],
   "source": [
    "# слово отделяется от другого любой последовательностью пробелов или пунктуации\n",
    "regex = re.compile('\\\\w+|[^\\\\w\\\\s]+')\n",
    "\n",
    "vocabulary = set()\n",
    "\n",
    "for anek in aneki:\n",
    "    vocabulary |= set(regex.findall(anek))\n",
    "\n",
    "print('Число уникальных слов:', len(vocabulary))\n",
    "# Output\n",
    "# Число уникальных слов: 159497\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e7b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, corpus, vocab_size=-1, n=2):\n",
    "        self.n = n\n",
    "        self.regex = re.compile('\\\\w+|[^\\\\w\\\\s]+')\n",
    "\n",
    "        words = []\n",
    "        for line in corpus:\n",
    "            # разбиваем по пробелам\n",
    "            words.extend(self.regex.findall(line))\n",
    "            \n",
    "        # считаем встречаемость слов\n",
    "        word_count = Counter(words)\n",
    "        if vocab_size == -1:  # если словарь не ограничен\n",
    "            self.vocab = set(word_count)\n",
    "        else:\n",
    "            common = word_count.most_common(vocab_size - 3)  # 3 специальных токена\n",
    "            self.vocab, _ = zip(*common)\n",
    "            self.vocab = set(self.vocab)\n",
    "\n",
    "        self.vocab |= set(['[UNK]', '[BOS]', '[EOS]'])\n",
    "\n",
    "    def encode(self, text):\n",
    "        # токенизируем\n",
    "        words = self.regex.findall(text)\n",
    "        encoded = [w if w in self.vocab else '[UNK]' for w in words]\n",
    "        # дополняем последовательность специальными токенами\n",
    "        encoded = ['[BOS]'] * self.n + encoded + ['[EOS]']\n",
    "        return WhitespaceOutput(encoded)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8f33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceOutput:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.ids = None\n",
    "\n",
    "\n",
    "def get_tokenizer(corpus, tokenizer_type, vocab_size=32768, n=2, lowercase=False):\n",
    "    assert tokenizer_type in ['whitespace', 'bpe', 'wordpiece', 'unigram']\n",
    "    \n",
    "    if tokenizer_type == 'whitespace':\n",
    "        return WhitespaceTokenizer(corpus, vocab_size=vocab_size, n=n)\n",
    "    if tokenizer_type == 'bpe':\n",
    "        model_class = BPE\n",
    "        trainer_class = BpeTrainer\n",
    "    elif tokenizer_type == 'wordpiece':\n",
    "        model_class = WordPiece\n",
    "        trainer_class = WordPieceTrainer\n",
    "    elif tokenizer_type == 'unigram':\n",
    "        model_class = Unigram\n",
    "        trainer_class = UnigramTrainer\n",
    "\n",
    "    if tokenizer_type == 'unigram':\n",
    "        # unk_token передается в Trainer\n",
    "        tokenizer = Tokenizer(model_class())\n",
    "    else:\n",
    "        tokenizer = Tokenizer(model_class(unk_token='[UNK]'))\n",
    "        # делим текст по пробелам (а так же символам пунктуации)\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    if lowercase:\n",
    "        # если надо, приводим текст к нижнему регистру\n",
    "        tokenizer.normalizer = Lowercase()\n",
    "\n",
    "    # указываем параметры токенизатора\n",
    "    trainer = trainer_class(\n",
    "        vocab_size=vocab_size,\n",
    "        # спецсимвол для указания, что токен является продолжением другого\n",
    "        continuing_subword_prefix='##',\n",
    "        special_tokens=['[BOS]', '[EOS]', '[UNK]', '[PAD]'],\n",
    "        unk_token='[UNK]'\n",
    "    )\n",
    "\n",
    "    # формируем файл с текстами\n",
    "    filename = \"%08x\" % random.getrandbits(32)\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in corpus:\n",
    "            f.write(f'{line}\\\\n')\n",
    "\n",
    "    tokenizer.train([filename], trainer)\n",
    "    os.remove(filename)\n",
    "    \n",
    "    if tokenizer_type != 'unigram':\n",
    "        # декодер, который склеивает токен с предыдущим, если в начале стоит ##\n",
    "        tokenizer.decoder = WordPieceDecoder(prefix='##', cleanup=True)\n",
    "    else:\n",
    "        # склеиваем все токены\n",
    "        tokenizer.decoder = WordPieceDecoder(prefix='', cleanup=True)\n",
    "\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b98cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option unk_token\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(aneki, 'bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb1f3e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Кто', 'сказал', ',', 'что', 'солдат', 'мечтает', 'стать', 'генералом', '?', 'Солдат', 'мечтает', 'стать', 'хлебо', '##рез', '##ом', '.']\n",
      "[1613, 1257, 13, 428, 5452, 7973, 1903, 20451, 31, 26223, 7973, 1903, 31628, 1982, 617, 15]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode('Кто сказал, что солдат мечтает стать генералом? Солдат мечтает стать хлеборезом.')\n",
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "\n",
    "# Output\n",
    "# ['Кто', 'сказал', ',', 'что', 'солдат', 'мечтает', 'стать', 'генералом', '?', 'Солдат', 'мечтает', 'стать', 'хлебо', '##рез', '##ом', '.']\n",
    "# [1333, 1232, 13, 422, 5377, 7868, 1873, 20185, 31, 12911, 7868, 1873, 31022, 1981, 625, 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f745c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Кто сказал, что солдат мечтает стать генералом? Солдат мечтает стать хлеборезом.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)\n",
    "\n",
    "# Output\n",
    "# 'Кто сказал, что солдат мечтает стать генералом? Солдат мечтает стать хлеборезом.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1159bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "def get_post_processor(n=2):\n",
    "    # post_processor добавит специальные токены в начало и конец последовательности\n",
    "    post_processor = TemplateProcessing(\n",
    "        single='[BOS] ' * n + '$A [EOS]',\n",
    "        special_tokens=[\n",
    "            (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "            (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
    "        ],\n",
    "    )\n",
    "    return post_processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21286b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', '[BOS]', 'Кто', 'сказал', ',', 'что', 'солдат', 'мечтает', 'стать', 'генералом', '?', 'Солдат', 'мечтает', 'стать', 'хлебо', '##рез', '##ом', '.', '[EOS]']\n",
      "[0, 0, 1613, 1257, 13, 428, 5452, 7973, 1903, 20451, 31, 26223, 7973, 1903, 31628, 1982, 617, 15, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.post_processor = get_post_processor(n=2)\n",
    "\n",
    "output = tokenizer.encode('Кто сказал, что солдат мечтает стать генералом? Солдат мечтает стать хлеборезом.')\n",
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "\n",
    "# Output\n",
    "# ['[BOS]', '[BOS]', 'Кто', 'сказал', ',', 'что', 'солдат', 'мечтает', 'стать', 'генералом', '?', 'Солдат', 'мечтает', 'стать', 'хлебо', '##рез', '##ом', '.', '[EOS]']\n",
    "# [0, 0, 1333, 1232, 13, 422, 5377, 7868, 1873, 20185, 31, 12911, 7868, 1873, 31022, 1981, 625, 15, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74ca1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def count_ngrams(aneki, tokenizer, n=2):\n",
    "    counts = defaultdict(Counter)\n",
    "\n",
    "    for anek in tqdm(aneki):\n",
    "        tokens = tokenizer.encode(anek).tokens\n",
    "\n",
    "        for i in range(n, len(tokens)):\n",
    "            prefix = tuple(tokens[i - n:i])  # префикс (n токенов)\n",
    "            token = tokens[i]  # последний токен\n",
    "            counts[prefix][token] += 1\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bf79246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Если бесконечное количество российских футболистов запустить на бесконечное количество футбольных полей и дать им бесконечное количество времени, то один из них когда-нибудь забьёт гол.',\n",
       " 'На чемпионат мира по футболу от России нужно Юлию Самойлову отправлять, хоть какая-то надежда на победу будет.',\n",
       " 'В целях профилактики от всего весной следует есть много чеснока. От женщин, кстати, тоже помогает.',\n",
       " 'На моих глазах как-то две девушки затаскивали кавказца в машину. Они худенькие, а он здоровый такой, никак не хотел в машину лезть. Они попросили у меня помощи, сказали, что собаку надо в ветклинику отвезти.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_lines = aneki[-4:]\n",
    "dummy_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42fd6552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405b0535e91149988bc4d79502f73d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'На': 2, 'Если': 1, 'В': 1})\n",
      "Counter({'.': 1, 'лезть': 1})\n"
     ]
    }
   ],
   "source": [
    "dummy_counts = count_ngrams(dummy_lines, tokenizer, n=2)\n",
    "\n",
    "print(dummy_counts[('[BOS]', '[BOS]')])\n",
    "print(dummy_counts[('в', 'машину')])\n",
    "\n",
    "# Output\n",
    "# Counter({'На': 2, 'Если': 1, 'В': 1})\n",
    "# Counter({'.': 1, 'лезть': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf8cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, tokenizer, n=2):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.post_processor = get_post_processor(n=n)\n",
    "        \n",
    "        self.counts = count_ngrams(corpus, tokenizer, n=n)\n",
    "        self.n = n\n",
    "\n",
    "        # посчитаем, сколько раз встречается каждый префикс\n",
    "        self.prefix_counts = dict()\n",
    "        for prefix, token_count in self.counts.items():\n",
    "            self.prefix_counts[prefix] = sum(token_count.values())\n",
    "\n",
    "    def get_last_n_tokens(self, prefix: Union[list, str]):\n",
    "        if isinstance(prefix, str):\n",
    "            prefix = self.tokenizer.encode(prefix).tokens[:-1]  # последний токен – [EOS]\n",
    "\n",
    "        return tuple(prefix[-self.n:])\n",
    "\n",
    "    def get_next_tokens_and_probs(self, prefix: Union[list, str]):\n",
    "        prefix = self.get_last_n_tokens(prefix)\n",
    "\n",
    "        possible_tokens = self.counts[prefix]  # dict <token: count>\n",
    "        \n",
    "        # возвращаем символ конца строки, если нет подходящих продолжений\n",
    "        if len(possible_tokens) == 0:\n",
    "            return ['[EOS]'], [1]\n",
    "\n",
    "        tokens = list(possible_tokens.keys())\n",
    "        \n",
    "        counts = list(possible_tokens.values())\n",
    "        probs = np.array(counts) / self.prefix_counts[prefix]\n",
    "\n",
    "        return tokens, probs\n",
    "\n",
    "    def get_token_prob(self, token, prefix):\n",
    "        prefix = self.get_last_n_tokens(prefix)\n",
    "        \n",
    "        possible_tokens = self.counts[prefix]  # dict <token: count>\n",
    "        token_count = possible_tokens.get(token, 0)\n",
    "        if token_count == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return token_count / self.prefix_counts[prefix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c293e447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc59f83324c40cfa0c7fe1b3091bfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = NGramLanguageModel(aneki, tokenizer, n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc90ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(lm, prefix):\n",
    "    tokens, probs = lm.get_next_tokens_and_probs(prefix)\n",
    "\n",
    "    next_token = np.random.choice(tokens, p=probs)\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1f68238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены: ['[BOS]', '[BOS]', '[BOS]', 'Приходит', 'парень', 'в', 'институт', ',', 'но', 'не', 'совсем', '.', 'Вы', '##ставил', 'бутылку', 'какого', '-', 'то', 'тай', '##ного', 'ор', '##дена', ',', 'который', 'бьется', 'за', 'то', ',', 'что', 'в', 'связи', 'с', 'провед', '##ением', 'ги', '##драв', '##ли', '##ческих', 'испыта', '##ний', 'она', 'бес', '##след', '##но', 'исчезают', 'вещи', '.', 'Видимо', ',', 'перем', '##ывают', 'косточки', 'птиц', 'улет', '##евших', 'зачем', '-', 'то', 'свою', 'игрушку', 'на', 'стол', 'и', 'при', '##п', '##уд', '##ривает', 'т', '##аль', '##ком', ',', 'говорит', 'ей', 'так', 'уча', '##стли', '##во', ':-', 'Женщина', ',', 'передайте', 'за', 'проезд', '!-', 'А', 'волшебное', 'слово', '?-', 'Б', '##егом', ',', 'твою', 'мать', '!', 'И', 'ведь', 'кто', '-', 'то', 'же', 'должен', 'держать', 'свечку', ',', 'чтобы']\n",
      "\n",
      "Текст: Приходит парень в институт, но не совсем. Выставил бутылку какого - то тайного ордена, который бьется за то, что в связи с проведением гидравлических испытаний она бесследно исчезают вещи. Видимо, перемывают косточки птиц улетевших зачем - то свою игрушку на стол и припудривает тальком, говорит ей так участливо :- Женщина, передайте за проезд!- А волшебное слово?- Бегом, твою мать! И ведь кто - то же должен держать свечку, чтобы\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "prefix = lm.tokenizer.encode('Приходит парень').tokens[:-1]  # последний токен – [EOS]\n",
    "\n",
    "for i in range(100):\n",
    "    prefix.append(get_next_token(lm, prefix))\n",
    "    if prefix[-1] == '[EOS]':\n",
    "        break\n",
    "\n",
    "print('Токены:', prefix)\n",
    "print()\n",
    "print('Текст:', lm.tokenizer.decode([lm.tokenizer.token_to_id(p) for p in prefix]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db00617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(lm, lines, min_prob=10 ** -50.):\n",
    "    # min_prob нужен, чтобы не брать логарифм 0,\n",
    "    # если в тестовой выборке есть новая n-грамма\n",
    "    ppls = []\n",
    "    for line in lines:\n",
    "        tokens = lm.tokenizer.encode(line).tokens\n",
    "        log_ppl = 0\n",
    "        for i in range(lm.n, len(tokens)):\n",
    "            log_ppl += np.log(max(\n",
    "                min_prob,\n",
    "                lm.get_token_prob(tokens[i], tokens[:i])\n",
    "            ))\n",
    "        text_len = len(tokens) - lm.n  # вычитаем число [BOS] токенов\n",
    "        ppls.append(np.exp(-log_ppl / text_len))\n",
    "\n",
    "    return np.mean(ppls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "327844f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa124fc085944f0e9bfb5abd086a5a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6762733f854be99cca1a716f60690a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6ae7a03a3d45be9fbe1391d4201490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities: ppx1=1114253017947795.875 ppx3=2427167859425.817 ppx10=1.049\n"
     ]
    }
   ],
   "source": [
    "lm1 = NGramLanguageModel(dummy_lines, tokenizer, n=1)\n",
    "lm3 = NGramLanguageModel(dummy_lines, tokenizer, n=3)\n",
    "lm10 = NGramLanguageModel(dummy_lines, tokenizer, n=10)\n",
    "\n",
    "ppx1 = perplexity(lm1, dummy_lines)\n",
    "ppx3 = perplexity(lm3, dummy_lines)\n",
    "ppx10 = perplexity(lm10, dummy_lines)\n",
    "\n",
    "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76609f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_lines, test_lines = train_test_split(aneki, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c9f04fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option unk_token\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# строим токенизатор ТОЛЬКО на ОБУЧАЮЩЕЙ выборке\n",
    "tokenizer = get_tokenizer(train_lines, 'bpe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee326b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c38866ae8f47e79a7dffb7623d122c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 2611138789471461499620342605283328.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbad62c6950e4546a1703bea715a1e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 3554294196110188807941063757886144821195702272.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e8f41798a74edaa727a1db144b7b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 234308237974753647225500111887375138263626940416.00000\n"
     ]
    }
   ],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(train_lines, tokenizer, n=n)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79197cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothNGramLM(NGramLanguageModel):\n",
    "    def __init__(self, corpus, tokenizer, n=2, delta=1.0):\n",
    "        super().__init__(corpus, tokenizer, n=n)\n",
    "        self.delta = delta\n",
    "        self.vocab = set(self.tokenizer.get_vocab())\n",
    "        self.n = n\n",
    "\n",
    "        self.smooth_probs = defaultdict(Counter)\n",
    "        # посчитаем сглаженные вероятности\n",
    "        for prefix, token_count in self.counts.items():\n",
    "            prefix_count = sum(token_count.values())\n",
    "            for token, count in token_count.items():\n",
    "                self.smooth_probs[prefix][token] = (count + delta) / (prefix_count + len(self.vocab) * delta)\n",
    "\n",
    "    def get_next_tokens_and_probs(self, prefix: Union[list, str]):\n",
    "        prefix = super().get_last_n_tokens(prefix)\n",
    "\n",
    "        token_probs = self.smooth_probs[prefix]  # dict <token: smooth_prob>\n",
    "\n",
    "        tokens = list(token_probs.keys())\n",
    "        probs = list(token_probs.values())\n",
    "        \n",
    "        # перераспределям оставшуюся вероятность\n",
    "        excess_prob = 1.0 - sum(probs)\n",
    "        unseen_tokens = self.vocab - set(tokens)\n",
    "        unseen_prob = excess_prob / (len(unseen_tokens) + 1e-6)  # деление на 0, если n=1\n",
    "\n",
    "        smooth_tokens = tokens + list(unseen_tokens)\n",
    "        smooth_probs = np.hstack((\n",
    "            probs,\n",
    "            np.full(len(unseen_tokens), fill_value=unseen_prob)  # массив одинаковых вероятностей\n",
    "        ))\n",
    "        return smooth_tokens, smooth_probs\n",
    "\n",
    "    def get_token_prob(self, token, prefix):        \n",
    "        prefix = super().get_last_n_tokens(prefix)\n",
    "        \n",
    "        token_probs = self.smooth_probs[prefix]\n",
    "        \n",
    "        prob = token_probs.get(token, 0)\n",
    "        if prob > 0:  # знаем вероятность для токена\n",
    "            return prob\n",
    "        else:  # не знаем вероятность для токена\n",
    "            excess_prob = 1.0 - sum(token_probs.values())\n",
    "            n_unseen_tokens = len(self.vocab) - len(token_probs)\n",
    "            return excess_prob / (n_unseen_tokens + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b385a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option unk_token\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_tokenizer = get_tokenizer(dummy_lines, 'bpe', vocab_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2672df83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10f39ce608545bb807bc301345ca340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d86aaf5f78459a9a87671d8a9fd62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2021e945326a40289093946509d96bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0360a51b4c1f4f8f9072a58a79510ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in (0, 1, 2, 3):\n",
    "    dummy_lm = SmoothNGramLM(dummy_lines, dummy_tokenizer, n=n)\n",
    "    token_probs = []\n",
    "    for token in dummy_lm.vocab:\n",
    "        token_probs.append(dummy_lm.get_token_prob(token, ['в']))\n",
    "\n",
    "    assert np.allclose(sum(token_probs), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bc21d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271317e117a448be9970b7a1431011fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 0, Perplexity = 29278.39106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f172de8a9a47279ceabaeca344bd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 1410.09250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f25ac5c368547389e22b60697257a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 8414.21721\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce50e0ac27d483a907b0d8b219056f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 16523.13916\n"
     ]
    }
   ],
   "source": [
    "for n in (0, 1, 2, 3):\n",
    "    lm = SmoothNGramLM(train_lines, tokenizer=tokenizer, n=n, delta=0.1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8bfa445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d750d2934d8450dad25de4eaba0c9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 0, Perplexity = 29278.29256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d68805c3cb49b4a44bea6b6919434d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 822.42003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ccc678bae8468a84b0df774fa2e2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 5399.91270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0da91c4cde4d79b19184c9824d69ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 13502.74112\n"
     ]
    }
   ],
   "source": [
    "for n in (0, 1, 2, 3):\n",
    "    lm = SmoothNGramLM(train_lines, tokenizer=tokenizer, n=n, delta=0.01)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0c059f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen_n_grams(lm, test_corpus):\n",
    "    number_unseen_n_grams = 0\n",
    "    \n",
    "    for line in test_corpus:\n",
    "        tokens = tuple(lm.tokenizer.encode(line).tokens[:-1])\n",
    "        \n",
    "        for i in range(lm.n, len(tokens)):\n",
    "            prefix = tuple(tokens[i - lm.n:i])\n",
    "            if len(lm.counts[prefix]) == 0:\n",
    "                number_unseen_n_grams += 1\n",
    "\n",
    "    return number_unseen_n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86a378af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e895c3cb304509bd09a0cd17c00f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 0, Perplexity = 29278.29, Unseen n-grams = 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db34a91d74aa4fba990e978ce7f37643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 822.42, Unseen n-grams = 73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fa9098e8eb42558141b1e16e0daa3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 5399.91, Unseen n-grams = 189263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e50ab95280b4f21b4218dade7008596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 13502.74, Unseen n-grams = 487060\n"
     ]
    }
   ],
   "source": [
    "for n in (0, 1, 2, 3):\n",
    "    lm = SmoothNGramLM(train_lines, tokenizer=tokenizer, n=n, delta=0.01)\n",
    "    ppl = perplexity(lm, test_lines)\n",
    "    unseen = unseen_n_grams(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.2f, Unseen n-grams = %i\" % (n, ppl, unseen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fd169d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3607a9a019884701878c068b0103a385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 0, Perplexity = 122302.09, Unseen n-grams = 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c42b85e4344635b05b63e494df04d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 4535.29, Unseen n-grams = 24232.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db88a4d0b4c4eae9fc77ff92a9f6e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 28506.18, Unseen n-grams = 212554.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485d516ad9d04bd4b6f52c54b5ab71d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 61237.36, Unseen n-grams = 425518.000\n"
     ]
    }
   ],
   "source": [
    "for n in (0, 1, 2, 3):\n",
    "    whitespace_tokenizer = get_tokenizer(train_lines, 'whitespace', vocab_size=-1, n=n)\n",
    "    lm = SmoothNGramLM(train_lines, tokenizer=whitespace_tokenizer, n=n, delta=0.01)\n",
    "    ppl = perplexity(lm, test_lines)\n",
    "    unseen = unseen_n_grams(lm, test_lines)\n",
    "    \n",
    "    print(\"N = %i, Perplexity = %.2f, Unseen n-grams = %.3f\" % (n, ppl, unseen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "721bba0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad70b37769148fb8e03013fcf30b7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 0, Perplexity = 27970.53, Unseen n-grams = 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377df6e7b7d54bce9d15b20997c04be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 546.18, Unseen n-grams = 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7608ee1689f45ada59eb55d48d82f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 3374.95, Unseen n-grams = 124445.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fef41a039b34725847719c2f423ce2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 10177.99, Unseen n-grams = 347710.000\n"
     ]
    }
   ],
   "source": [
    "for n in (0, 1, 2, 3):\n",
    "    whitespace_tokenizer = get_tokenizer(train_lines, 'whitespace', vocab_size=32768, n=n)\n",
    "    lm = SmoothNGramLM(train_lines, tokenizer=whitespace_tokenizer, n=n, delta=0.01)\n",
    "    ppl = perplexity(lm, test_lines)\n",
    "    unseen = unseen_n_grams(lm, test_lines)\n",
    "    \n",
    "    print(\"N = %i, Perplexity = %.2f, Unseen n-grams = %.3f\" % (n, ppl, unseen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14602968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option unk_token\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(train_lines, 'wordpiece')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "694f664d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f8e40770ba4ba9b60a8d3674f58497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 0, Perplexity = 29419.16, Unseen n-grams = 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f32b82abdbe443b83e9dd5a18c8a32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 824.35, Unseen n-grams = 144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179b974deb5649aa9e1523458ff8f802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 5361.20, Unseen n-grams = 189286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6a8c951c474d45b9a5a34c80317644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 13535.41, Unseen n-grams = 484805\n"
     ]
    }
   ],
   "source": [
    "for n in (0, 1, 2, 3):\n",
    "    lm = SmoothNGramLM(train_lines, tokenizer=tokenizer, n=n, delta=0.01)\n",
    "    ppl = perplexity(lm, test_lines)\n",
    "    unseen = unseen_n_grams(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.2f, Unseen n-grams = %i\" % (n, ppl, unseen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe5207f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m(train_lines, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munigram\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(train_lines, 'unigram')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
