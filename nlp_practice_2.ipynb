{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f052ee67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb  # будем использовать для логирования\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "wandb.login()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95daded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login cc603ae0565bbbfce5cc5b068a9bddabb8950920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e212aa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 7600)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('data/train.csv', sep=',')\n",
    "data_test = pd.read_csv('data/test.csv', sep=',')\n",
    "\n",
    "# Выделяем целевые переменные\n",
    "labels_train = data_train['label']\n",
    "labels_test = data_test['label']\n",
    "\n",
    "# Смотрим на размер данных\n",
    "len(data_train), len(data_test)\n",
    "# Output:\n",
    "# (120000, 7600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08184e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Wall St. Bears Claw Back Into the Black (Reute...\n",
       "1    Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2    Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
       "3    Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4    Oil prices soar to all-time record, posing new...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d026b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    reg = re.compile(r'\\w+')\n",
    "    return reg.findall(text)\n",
    "    \n",
    "# Применяем функцию токенизации текстов к тренировочной и тестовой выборкам    \n",
    "data_tok_train = [tokenize(t.lower()) for t in data_train['text']]\n",
    "data_tok_test = [tokenize(t.lower()) for t in data_test['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edc3738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(tokenized_texts):\n",
    "    clear_texts = []\n",
    "    for words in tokenized_texts:\n",
    "        clear_texts.append([word for word in words if word not in stop_words])\n",
    "\n",
    "    return clear_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8d7647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tok_train = remove_stopwords(data_tok_train)\n",
    "data_tok_test = remove_stopwords(data_tok_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e89ee91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7fc5e0a4e6423f824e04e5cc149347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ffad4af94c4bca8a527186dcad558e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lemmatize_text(tokenized_texts):\n",
    "    lemmatized_data = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for words in tqdm(tokenized_texts):\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        lemmatized_data.append(lemmatized_words)\n",
    "    return lemmatized_data\n",
    "    \n",
    "    \n",
    "lemmatized_train = lemmatize_text(data_tok_train)\n",
    "lemmatized_test = lemmatize_text(data_tok_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
