{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a092cb3-5b2e-4fb1-bc0c-23e552857aec",
      "metadata": {
        "id": "2a092cb3-5b2e-4fb1-bc0c-23e552857aec"
      },
      "source": [
        "# Урок 9. Часть 3\n",
        "\n",
        "1. Арендуем компьютер с ГПУ в облаке и настраиваем его.\n",
        "1. Запускаем API сервер с инференсом LLM (деплоим LLM).\n",
        "1. Запускаем веб интерфейс для чата с запущенной LLM (показываем LLM)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b758986b-9547-400f-88bf-7c157925e62e",
      "metadata": {
        "id": "b758986b-9547-400f-88bf-7c157925e62e"
      },
      "source": [
        "## Аренда компьютера\n",
        "\n",
        "https://my.selectel.ru/\n",
        "https://docs.selectel.ru/cloud/servers/manage/connect-to-server/\n",
        "\n",
        "![](gpu/1.png)\n",
        "*****\n",
        "![](gpu/2.png)\n",
        "*****\n",
        "![](gpu/3.png)\n",
        "*****\n",
        "![](gpu/4.png)\n",
        "*****\n",
        "![](gpu/5.png)\n",
        "*****\n",
        "![](gpu/6.png)\n",
        "*****\n",
        "![](gpu/7.png)\n",
        "*****\n",
        "![](gpu/8.png)\n",
        "*****\n",
        "![](gpu/9.png)\n",
        "*****"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f97d2b83-a404-40c3-8eb5-f465811ab90c",
      "metadata": {
        "id": "f97d2b83-a404-40c3-8eb5-f465811ab90c"
      },
      "source": [
        "## Деплой LLM\n",
        "```bash\n",
        "# Устанавливаем необходимые пакеты\n",
        "apt install git unzip\n",
        "\n",
        "# Создаем новое виртуальное окружение для нашего инференса LLM\n",
        "conda create -n llmenv python=3.10 -y\n",
        "conda activate llmenv\n",
        "\n",
        "# Устанавливаем vllm для CUDA 11.8\n",
        "export VLLM_VERSION=0.4.2\n",
        "export PYTHON_VERSION=310\n",
        "pip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "pip install flash_attn\n",
        "\n",
        "# Запускаем инференс LLM\n",
        "python -m vllm.entrypoints.openai.api_server --model solidrust/Hermes-2-Pro-Llama-3-8B-AWQ --dtype auto --api-key token-abc123 --served-model-name llm\n",
        "```\n",
        "\n",
        "- Ссылка на модель: https://huggingface.co/solidrust/Hermes-2-Pro-Llama-3-8B-AWQ\n",
        "- Ссылка на VLLM: https://docs.vllm.ai/en/stable/, https://github.com/vllm-project/vllm\n",
        "- Ссылка на VLLM документацию сервера: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#command-line-arguments-for-the-server"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20919712-5773-41a9-bd3d-e066e870b691",
      "metadata": {
        "id": "20919712-5773-41a9-bd3d-e066e870b691"
      },
      "source": [
        "## Деплой LLM Web Interface\n",
        "https://github.com/reflex-dev/reflex-chat\n",
        "\n",
        "```bash\n",
        "\n",
        "# Создаем новое виртуальное окружение для чатбота\n",
        "conda create -n chatbotenv python=3.10 -y\n",
        "conda activate chatbotenv\n",
        "\n",
        "# Устанавливаем нужные библиотеки\n",
        "pip install reflex\n",
        "\n",
        "# Создаем папку для проекта и переходим туда\n",
        "mkdir chat\n",
        "cd chat\n",
        "\n",
        "# Инициализируем проект с веб интерфейсом чатилкой (https://github.com/reflex-dev/reflex-chat)\n",
        "reflex init --template chat --loglevel debug\n",
        "\n",
        "# Доустанавливаем нужные библиотеки\n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671dba96-a85f-4a62-8202-19bcc42e1a1f",
      "metadata": {
        "id": "671dba96-a85f-4a62-8202-19bcc42e1a1f"
      },
      "source": [
        "Редактируем код, чтобы он ходил в локальный LLM сервер, а не в chatgpt:\n",
        "\n",
        "```bash\n",
        "cd chat\n",
        "nano state.py\n",
        "```\n",
        "\n",
        "```python\n",
        "# Добавляем строчку после импортов\n",
        "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\")\n",
        "\n",
        "# В функции openai_process_question меняем код\n",
        "# Было\n",
        "session = OpenAI().chat.completions.create(\n",
        "# Стало\n",
        "session = client.chat.completions.create(\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a98a5f-1148-47ea-9e7b-b0e2d334e013",
      "metadata": {
        "id": "58a98a5f-1148-47ea-9e7b-b0e2d334e013"
      },
      "source": [
        "```bash\n",
        "# Запускаем веб интерфейс\n",
        "export OPENAI_API_KEY=\"token-abc123\"\n",
        "export OPENAI_MODEL=\"llm\"\n",
        "reflex run --loglevel debug\n",
        "```\n",
        "\n",
        "Открываем в браузере `http://SERVER_URL:3000/`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4cf0df7-4e40-4cbb-9ff3-1fd86edad958",
      "metadata": {
        "id": "b4cf0df7-4e40-4cbb-9ff3-1fd86edad958"
      },
      "source": [
        "### Дальнейшие шаги\n",
        "- Сделать настройку системного промпта и параметров генерации\n",
        "- Добавить удаление/редактирование сообщений, копирование текста\n",
        "- Переписать код целиком чтобы лучше разобраться в том как все устроено, улучшить дизайн, оптимизировать запросы\n",
        "- Добавить заранее заготовленные промпты для пользователей\n",
        "- Добавить использование инструментов LLM (агент)\n",
        "- ...\n",
        "\n",
        "\n",
        "*****\n",
        "- Упаковать все в Docker\n",
        "- Забрать все переменные (адрес LLM сервера, ключ от API, название LLM) в конфиг\n",
        "- Попробовать другой фреймворк для запуска LLM сервера: https://github.com/huggingface/text-generation-inference, https://huggingface.co/docs/text-generation-inference/index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a89a2d1c-f7ac-46fb-9f8a-ce93d73eacc1",
      "metadata": {
        "id": "a89a2d1c-f7ac-46fb-9f8a-ce93d73eacc1"
      },
      "source": [
        "## Резюме\n",
        "\n",
        "1. Арендовали компьютер с ГПУ в облаке\n",
        "2. Запустили на компьютере API сервер с инференсом LLM\n",
        "3. Запустили на компьютере веб интерфейс для чата с LLM\n",
        "4. Получили заготовку для пет проекта"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}