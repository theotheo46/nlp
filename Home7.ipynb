{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 7\n",
    "\n",
    "Сегодня будем решать задачу _машинного перевода_ с помощью RNN.\n",
    "\n",
    "1. Построим RNN, обучим на текстах.\n",
    "2. Построим bi-directional RNN, обучим, сравним качество.\n",
    "\n",
    "Стоит отметить, что RNN - это не самый популярный и надежный метод из-за проблем с затуханием и взрывом градиентов, а также ограниченной способности захватывать долгосрочные зависимости в тексте.\n",
    "Более улучшенные и эффективные модели перевода (такие как LSTM, GRU и трансформеры) вы узнаете в блоке NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T19:56:16.962849Z",
     "iopub.status.busy": "2024-05-28T19:56:16.962206Z",
     "iopub.status.idle": "2024-05-28T19:56:20.635152Z",
     "shell.execute_reply": "2024-05-28T19:56:20.634345Z",
     "shell.execute_reply.started": "2024-05-28T19:56:16.962802Z"
    },
    "id": "G-4bDyfYu3BC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1hXxzOfHu3BC"
   },
   "outputs": [],
   "source": [
    "# Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T19:56:20.636970Z",
     "iopub.status.busy": "2024-05-28T19:56:20.636564Z",
     "iopub.status.idle": "2024-05-28T19:56:21.582720Z",
     "shell.execute_reply": "2024-05-28T19:56:21.581791Z",
     "shell.execute_reply.started": "2024-05-28T19:56:20.636944Z"
    },
    "id": "AL-YzZNUu3BD"
   },
   "outputs": [],
   "source": [
    "def load_pairs(file_path):\n",
    "    pairs = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            pair = line.strip().split(\"\\t\")\n",
    "            pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "\n",
    "pairs = load_pairs(\"pairs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T19:57:07.237559Z",
     "iopub.status.busy": "2024-05-28T19:57:07.236592Z",
     "iopub.status.idle": "2024-05-28T19:57:07.244051Z",
     "shell.execute_reply": "2024-05-28T19:57:07.243121Z",
     "shell.execute_reply.started": "2024-05-28T19:57:07.237521Z"
    },
    "id": "Cv7tYoJPu3BD",
    "outputId": "e3be27b0-0a02-43ce-f175-3cdd7bd5ef2b"
   },
   "outputs": [],
   "source": [
    "pairs[200:205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMZ475Wgu3BD"
   },
   "outputs": [],
   "source": [
    "# Делаем нужные предобработки для задачи перевода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T19:57:10.935790Z",
     "iopub.status.busy": "2024-05-28T19:57:10.935464Z",
     "iopub.status.idle": "2024-05-28T19:57:12.225996Z",
     "shell.execute_reply": "2024-05-28T19:57:12.224981Z",
     "shell.execute_reply.started": "2024-05-28T19:57:10.935766Z"
    },
    "id": "XUmAKwBWu3BD",
    "outputId": "aaf67342-75e9-477e-93ed-5b2ec6d3b9fc"
   },
   "outputs": [],
   "source": [
    "# Определение специальных токенов\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "\n",
    "# Функция токенизации предложения: приводит все символы к нижнему регистру и разбивает предложение на слова\n",
    "def tokenize(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "\n",
    "# Функция для построения словарей для английских и русских слов на основе пар предложений\n",
    "def build_vocab(pairs):\n",
    "    eng_vocab = set()\n",
    "    rus_vocab = set()\n",
    "    for eng_sentence, rus_sentence in pairs:\n",
    "        eng_vocab.update(tokenize(eng_sentence))\n",
    "        rus_vocab.update(tokenize(rus_sentence))\n",
    "    return eng_vocab, rus_vocab\n",
    "\n",
    "\n",
    "# Функция для создания отображений слово -> индекс и индекс -> слово\n",
    "def create_mappings(vocab):\n",
    "    vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted(vocab)\n",
    "    word2int = {word: i for i, word in enumerate(vocab)}\n",
    "    int2word = {i: word for word, i in word2int.items()}\n",
    "    return word2int, int2word\n",
    "\n",
    "\n",
    "# Создание словарей для английских и русских предложений на основе пар\n",
    "english_vocab, russian_vocab = build_vocab(pairs)\n",
    "\n",
    "# Создание отображений с добавлением специальных токенов\n",
    "eng_word2int, eng_int2word = create_mappings(english_vocab)\n",
    "rus_word2int, rus_int2word = create_mappings(russian_vocab)\n",
    "\n",
    "# Печать размеров словарей (с учетом 4 специальных токенов)\n",
    "print(\"English vocabulary size:\", len(english_vocab) + 4)\n",
    "print(\"Russian vocabulary size:\", len(russian_vocab) + 4)\n",
    "\n",
    "# Пример использования: кодирование английского и русского предложения\n",
    "eng_example = \"Who are you\"\n",
    "rus_example = \"как ты\"\n",
    "\n",
    "# Кодирование с учетом UNK_TOKEN для неизвестных слов\n",
    "eng_encoded = np.array(\n",
    "    [eng_word2int.get(word, eng_word2int[UNK_TOKEN]) for word in tokenize(eng_example)],\n",
    "    dtype=np.int32,\n",
    ")\n",
    "rus_encoded = np.array(\n",
    "    [rus_word2int.get(word, rus_word2int[UNK_TOKEN]) for word in tokenize(rus_example)],\n",
    "    dtype=np.int32,\n",
    ")\n",
    "\n",
    "print(\"English text encoded:\", eng_encoded)\n",
    "print(\"Russian text encoded:\", rus_encoded)\n",
    "\n",
    "# Декодирование: восстановление текста из кодов\n",
    "decoded_eng = \" \".join([eng_int2word[i] for i in eng_encoded])\n",
    "decoded_rus = \" \".join([rus_int2word[i] for i in rus_encoded])\n",
    "\n",
    "print(\"Decoded English:\", decoded_eng)\n",
    "print(\"Decoded Russian:\", decoded_rus)\n",
    "\n",
    "\n",
    "# Определение класса датасета для перевода\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, eng_word2int, rus_word2int):\n",
    "        self.pairs = pairs\n",
    "        self.eng_word2int = eng_word2int\n",
    "        self.rus_word2int = rus_word2int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng, rus = self.pairs[idx]\n",
    "        # Кодирование английского предложения и добавление EOS токена\n",
    "        eng_tensor = torch.tensor(\n",
    "            [\n",
    "                self.eng_word2int.get(word, self.eng_word2int[UNK_TOKEN])\n",
    "                for word in tokenize(eng)\n",
    "            ]\n",
    "            + [self.eng_word2int[EOS_TOKEN]],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        # Кодирование русского предложения и добавление EOS токена\n",
    "        rus_tensor = torch.tensor(\n",
    "            [\n",
    "                self.rus_word2int.get(word, self.rus_word2int[UNK_TOKEN])\n",
    "                for word in tokenize(rus)\n",
    "            ]\n",
    "            + [self.rus_word2int[EOS_TOKEN]],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        return eng_tensor, rus_tensor\n",
    "\n",
    "\n",
    "# Функция для объединения батчей: паддинг (дополнение) предложений до одной длины в батче\n",
    "def collate_fn(batch):\n",
    "    eng_batch, rus_batch = zip(*batch)\n",
    "    eng_batch_padded = pad_sequence(\n",
    "        eng_batch, batch_first=True, padding_value=eng_word2int[PAD_TOKEN]\n",
    "    )\n",
    "    rus_batch_padded = pad_sequence(\n",
    "        rus_batch, batch_first=True, padding_value=rus_word2int[PAD_TOKEN]\n",
    "    )\n",
    "    return eng_batch_padded, rus_batch_padded\n",
    "\n",
    "\n",
    "# Создание экземпляра датасета и загрузчика данных\n",
    "translation_dataset = TranslationDataset(pairs, eng_word2int, rus_word2int)\n",
    "batch_size = 64\n",
    "translation_dataloader = DataLoader(\n",
    "    translation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# Печать информации о количестве образцов и батчей в датасете\n",
    "print(\"Translation samples: \", len(translation_dataset))\n",
    "print(\"Translation batches: \", len(translation_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простая RNN\n",
    "\n",
    "Начнем свои эксперименты с простой `RNN` - без bidirectional и с одним слоем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №1\n",
    "\n",
    "Добавьте недостающие части в класс `Encoder` и сдайте в ЛМС код класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T19:57:42.835960Z",
     "iopub.status.busy": "2024-05-28T19:57:42.835612Z",
     "iopub.status.idle": "2024-05-28T19:57:42.843224Z",
     "shell.execute_reply": "2024-05-28T19:57:42.842128Z",
     "shell.execute_reply.started": "2024-05-28T19:57:42.835935Z"
    },
    "id": "UGoIzfq9u3BE"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Добавьте слой RNN\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flip(x, [1])\n",
    "        embedded = self.embedding(x)\n",
    "        ...\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №2\n",
    "\n",
    "Добавьте недостающий код в `Decoder` и сдайте в ЛМС код класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:00:32.342948Z",
     "iopub.status.busy": "2024-05-28T20:00:32.342066Z",
     "iopub.status.idle": "2024-05-28T20:00:32.349694Z",
     "shell.execute_reply": "2024-05-28T20:00:32.348816Z",
     "shell.execute_reply.started": "2024-05-28T20:00:32.342912Z"
    },
    "id": "0dD04WFdu3BE"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Добавьте слой RNN и линейный слой, который преобразует выход RNN в размер словаря\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: torch.Tensor | None):\n",
    "        out = self.embedding(x)\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:00:38.650662Z",
     "iopub.status.busy": "2024-05-28T20:00:38.650292Z",
     "iopub.status.idle": "2024-05-28T20:00:38.693473Z",
     "shell.execute_reply": "2024-05-28T20:00:38.692512Z",
     "shell.execute_reply.started": "2024-05-28T20:00:38.650636Z"
    },
    "id": "J7lErRJSu3BE",
    "outputId": "c0dc0dba-e855-4776-ab62-1da726ab7367"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:00:43.769700Z",
     "iopub.status.busy": "2024-05-28T20:00:43.769008Z",
     "iopub.status.idle": "2024-05-28T20:00:44.913843Z",
     "shell.execute_reply": "2024-05-28T20:00:44.913028Z",
     "shell.execute_reply.started": "2024-05-28T20:00:43.769668Z"
    },
    "id": "6s5M_mohu3BF"
   },
   "outputs": [],
   "source": [
    "eng_vocab_size = len(eng_word2int)\n",
    "rus_vocab_size = len(rus_word2int)\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "encoder = Encoder(eng_vocab_size, embed_size, hidden_size, num_layers).to(DEVICE)\n",
    "decoder = Decoder(rus_vocab_size, embed_size, hidden_size, num_layers).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:00:48.182086Z",
     "iopub.status.busy": "2024-05-28T20:00:48.181200Z",
     "iopub.status.idle": "2024-05-28T20:00:48.192055Z",
     "shell.execute_reply": "2024-05-28T20:00:48.191001Z",
     "shell.execute_reply.started": "2024-05-28T20:00:48.182053Z"
    },
    "id": "UX4XduJ5u3BF"
   },
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, sentence, eng_word2int, rus_int2word, max_length=15):\n",
    "    # Переводим модели в режим оценки (inference)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Отключаем вычисление градиентов для ускорения и уменьшения использования памяти\n",
    "    with torch.inference_mode():\n",
    "        # Преобразуем входное предложение в тензор и добавляем EOS токен в конце\n",
    "        input_tensor = torch.tensor(\n",
    "            [eng_word2int[word] for word in tokenize(sentence)]\n",
    "            + [eng_word2int[EOS_TOKEN]],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        input_tensor = input_tensor.view(1, -1).to(DEVICE)  # batch_first=True\n",
    "\n",
    "        # Пропускаем входное предложение через энкодер\n",
    "        _, encoder_hidden = encoder(input_tensor)\n",
    "        # Инициализируем скрытое состояние декодера скрытым состоянием энкодера\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        last_word = torch.tensor([[eng_word2int[SOS_TOKEN]]]).to(DEVICE)\n",
    "        for _ in range(max_length):\n",
    "            # Пропускаем последний предсказанный токен через декодер\n",
    "            logits, decoder_hidden = decoder(last_word, decoder_hidden)\n",
    "            # Жадный перебор: выбираем токен с максимальной вероятностью - можно было и с температурой, попробуйте в качестве эксперименте\n",
    "            next_token = logits.argmax(dim=1)\n",
    "            last_word = next_token.unsqueeze(0).to(DEVICE)\n",
    "            if next_token.item() == rus_word2int[EOS_TOKEN]:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(rus_int2word.get(next_token.item()))\n",
    "\n",
    "    # Возвращаем переведенные слова как строку\n",
    "    return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nYbhva1Uu3BF"
   },
   "outputs": [],
   "source": [
    "sentence = \"just do it\"\n",
    "translated_sentence = translate(encoder, decoder, sentence, eng_word2int, rus_int2word)\n",
    "print(\"Translated:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока перевод получается странный, но мы ведь пока ничего не обучали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f1Z7eyT-u3BF"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Функция потерь (исключая паддинг)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=eng_word2int[PAD_TOKEN])\n",
    "\n",
    "# Оптимизаторы\n",
    "encoder_optimizer = optim.AdamW(encoder.parameters())\n",
    "decoder_optimizer = optim.AdamW(decoder.parameters())\n",
    "\n",
    "# Количество эпох\n",
    "num_epochs = 1\n",
    "\n",
    "# Тренировочный цикл\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_tensor, target_tensor) in enumerate(translation_dataloader):\n",
    "        input_tensor, target_tensor = input_tensor.to(DEVICE), target_tensor.to(DEVICE)\n",
    "\n",
    "        # Обнуление градиентов обоих оптимизаторов\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        target_length = target_tensor.size(1)\n",
    "\n",
    "        # Энкодер\n",
    "        _, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        # Декодер\n",
    "        decoder_input = torch.full(\n",
    "            (batch_size, 1), eng_word2int[SOS_TOKEN], dtype=torch.long\n",
    "        ).to(DEVICE)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Случайный выбор индекса слова из целевой последовательности\n",
    "        random_word_index = random.randint(0, target_length - 1)\n",
    "\n",
    "        loss = torch.tensor(0.0, device=DEVICE, requires_grad=True)\n",
    "        for di in range(target_length):\n",
    "            logits, _ = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # Вычисление потерь только для случайно выбранного слова\n",
    "            loss = loss + loss_fn(logits, target_tensor[:, di])\n",
    "\n",
    "            decoder_input = target_tensor[:, di].reshape(\n",
    "                batch_size, 1\n",
    "            )  # Teacher forcing (принудительное обучение)\n",
    "\n",
    "        # Обратное распространение ошибки и шаг оптимизации\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # Печать потерь каждые 100 батчей\n",
    "            print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item() / target_length:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №3\n",
    "Попробуйте перевести предложение \"Where is Tom?\".\n",
    "Сдайте в ЛМС перевод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xQPOURtUu3BF"
   },
   "outputs": [],
   "source": [
    "sentence = \"Where is Tom?\"\n",
    "translated_sentence = translate(encoder, decoder, sentence, eng_word2int, rus_int2word)\n",
    "print(\"Translated:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN\n",
    "\n",
    "Теперь попробуем использовать двунаправленную RNN (bidirectional RNN) в энкодере,\n",
    "что позволяет модели учитывать информацию из обеих сторон последовательности — как слева направо, так и справа налево.\n",
    "\n",
    "Декодер остается односторонним."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №4\n",
    "\n",
    "Допишите недостающий код в `Encoder` и сдайте на в ЛМС код класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:06:36.477125Z",
     "iopub.status.busy": "2024-05-28T20:06:36.476475Z",
     "iopub.status.idle": "2024-05-28T20:06:36.484711Z",
     "shell.execute_reply": "2024-05-28T20:06:36.483647Z",
     "shell.execute_reply.started": "2024-05-28T20:06:36.477091Z"
    },
    "id": "oNUR7MXVu3BG"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Добавьте двунаправленную RNN\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # Двунаправленная RNN возвращает два скрытых состояния: одно для каждого направления.\n",
    "        # Объединяем их в одно скрытое состояние.\n",
    "        hidden = torch.cat((hidden[0, :, :], hidden[1, :, :]), dim=1).unsqueeze(0)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №5\n",
    "\n",
    "Допишите класс `Decoder` и сдайте в ЛМС его реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:05:53.495561Z",
     "iopub.status.busy": "2024-05-28T20:05:53.494951Z",
     "iopub.status.idle": "2024-05-28T20:05:53.502516Z",
     "shell.execute_reply": "2024-05-28T20:05:53.501481Z",
     "shell.execute_reply.started": "2024-05-28T20:05:53.495530Z"
    },
    "id": "rNc365mJu3BG"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        ...\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out = self.embedding(x)\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:06:43.698999Z",
     "iopub.status.busy": "2024-05-28T20:06:43.698366Z",
     "iopub.status.idle": "2024-05-28T20:06:45.017536Z",
     "shell.execute_reply": "2024-05-28T20:06:45.016514Z",
     "shell.execute_reply.started": "2024-05-28T20:06:43.698967Z"
    },
    "id": "-dH3nKy2u3BG"
   },
   "outputs": [],
   "source": [
    "eng_vocab_size = len(eng_word2int)\n",
    "ita_vocab_size = len(rus_word2int)\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "\n",
    "encoder = Encoder(eng_vocab_size, embed_size, hidden_size, num_layers).to(DEVICE)\n",
    "decoder = Decoder(ita_vocab_size, embed_size, hidden_size * 2, num_layers).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kYMrVqLwu3BG"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=eng_word2int[PAD_TOKEN])\n",
    "\n",
    "encoder_optimizer = optim.AdamW(encoder.parameters())\n",
    "decoder_optimizer = optim.AdamW(decoder.parameters())\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_tensor, target_tensor) in enumerate(translation_dataloader):\n",
    "        input_tensor, target_tensor = input_tensor.to(DEVICE), target_tensor.to(DEVICE)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        target_length = target_tensor.size(1)\n",
    "\n",
    "        _, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        decoder_input = torch.full(\n",
    "            (batch_size, 1), eng_word2int[SOS_TOKEN], dtype=torch.long\n",
    "        ).to(DEVICE)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        random_word_index = random.randint(0, target_length - 1)\n",
    "\n",
    "        loss = torch.tensor(0.0, device=DEVICE, requires_grad=True)\n",
    "        for di in range(target_length):\n",
    "            logits, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            loss = loss + loss_fn(logits, target_tensor[:, di])\n",
    "\n",
    "            decoder_input = target_tensor[:, di].reshape(batch_size, 1)\n",
    "\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item() / target_length:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_EpollrXu3BG"
   },
   "outputs": [],
   "source": [
    "sentence = \"just do it\"\n",
    "translated_sentence = translate(encoder, decoder, sentence, eng_word2int, rus_int2word)\n",
    "print(\"Translated:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбить хорошее качество обучения, используя только лишь RNN и небольшой датасет, сложно.\n",
    "\n",
    "Не забывайте, что в семинаре у нас было ~400 Мб текстов одного языка, а здесь всего лишь 28 Мб и на двух языках.\n",
    "Можем сделать вывод, что для обучения хорошей модели нужно много текстовых данных.\n",
    "\n",
    "Помимо этого, для серьезного обучения стоит использовать более продвинутые сети: те же GRU и LSTM покажут себя лучше.\n",
    "А еще лучше будет работать трансформер, о котором вы узнаете в следующем уроке."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5101699,
     "sourceId": 8539972,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "start-dl-nJVKNwkY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}